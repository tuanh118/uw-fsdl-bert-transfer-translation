{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('fsdl-text-recognizer': conda)",
      "language": "python",
      "name": "python37764bitfsdltextrecognizercondadeddc9485e524fe5bef8d396f447eae4"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmNVcTiAhwfD",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tuanh118/uw-fsdl-bert-transfer-translation/blob/add-collab-notebook/Notebook.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCjQJgSsfzWg",
        "colab_type": "text"
      },
      "source": [
        "## Set up the Collab VM and import libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKPCWTgXfzWX",
        "colab_type": "code",
        "outputId": "5d2a0783-a5d1-4da9-b3f9-1d273c7c113e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!rm -rf uw-fsdl-bert-transfer-translation\n",
        "\n",
        "# Hack to reference other files in the repo in Collab.\n",
        "!git clone https://github.com/tuanh118/uw-fsdl-bert-transfer-translation -b tensorflow-rewrite\n",
        "import sys\n",
        "sys.path.append('./uw-fsdl-bert-transfer-translation')\n",
        "\n",
        "# Install required packages.\n",
        "!pip install transformers\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "\n",
        "from CombinedBertTransformerModel import *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Cloning into 'uw-fsdl-bert-transfer-translation'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 52 (delta 14), reused 35 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtvhmVW8hAdv",
        "colab_type": "text"
      },
      "source": [
        "## Data retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKiRA9jVfzWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the EuroParl French-English corpus.\n",
        "path_to_fr_en_tar = tf.keras.utils.get_file('fr-en.tgz', origin='https://www.statmt.org/europarl/v7/fr-en.tgz', extract=True)\n",
        "path_to_fr_en_en_file = os.path.dirname(path_to_fr_en_tar) + \"/europarl-v7.fr-en.en\"\n",
        "path_to_fr_en_fr_file = os.path.dirname(path_to_fr_en_tar) + \"/europarl-v7.fr-en.fr\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wlQGXSx-7tj_"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "21bdc9c1-40d6-47d2-82c4-23770dfc471f",
        "id": "jG76mcdq7tkB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Sets up a BERT tokenizer.\n",
        "def instantiate_tokenizer():\n",
        "    return BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Given a path to a text file, load and tokenize each line using the provided tokenizer, then convert each token to an ID and pad all lines to have length max_tokens.\n",
        "def load_dataset(language_path, tokenizer, num_examples=None, max_tokens=500):\n",
        "    # Read the data.\n",
        "    lines = io.open(language_path, encoding='UTF-8').read().strip().splitlines()[:num_examples]\n",
        "\n",
        "    # Tokenize and add the special start token.\n",
        "    tokenized_lines = [ ['[CLS]'] + tokenizer.tokenize(line)[:max_tokens-1] + ['[SEP]'] for line in lines ]\n",
        "    \n",
        "    # Convert tokens to IDs.\n",
        "    ids = [ tokenizer.convert_tokens_to_ids(tokenized_line) for tokenized_line in tokenized_lines ]\n",
        "\n",
        "    # Generate padding masks and segment IDs. These have the same length as the ID sequences after padding.\n",
        "    # Padding mask is 1 where there is an actual ID and 0 where there is padding. Segment ID is always 0.\n",
        "    masks = [ [1] * len(tokenized_line) for tokenized_line in tokenized_lines ]\n",
        "    segments = [ [] for tokenized_line in tokenized_lines ]\n",
        "\n",
        "    # Pad all ID sequences to the maximum length with zeroes.\n",
        "    ids = tf.keras.preprocessing.sequence.pad_sequences(ids, maxlen=max_tokens, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "    masks = tf.keras.preprocessing.sequence.pad_sequences(masks, maxlen=max_tokens, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "    segments = tf.keras.preprocessing.sequence.pad_sequences(segments, maxlen=max_tokens, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "    return ids, masks, segments\n",
        "\n",
        "num_examples = 300\n",
        "max_tokens = 50\n",
        "tokenizer = instantiate_tokenizer()\n",
        "input_tensor, masks, segments = load_dataset(path_to_fr_en_en_file, tokenizer, num_examples, max_tokens)\n",
        "target_tensor, _, _ = load_dataset(path_to_fr_en_fr_file, tokenizer, num_examples, max_tokens)\n",
        "\n",
        "# Split the data into training and validation sets.  No test set for now since we're just experimenting.\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Do some printing to show that the processing worked.\n",
        "def convert(tokenizer, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print (\"%d ----> %s\" % (t, tokenizer.ids_to_tokens[t]))\n",
        "\n",
        "print(\"ID to token mapping for first training example (input)\")\n",
        "convert(tokenizer, input_tensor_train[0])\n",
        "print()\n",
        "print(\"ID to token mapping for first training example (target)\")\n",
        "convert(tokenizer, target_tensor_train[0])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 32\n",
        "units = 32\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(len(input_tensor_train)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(len(input_tensor_val)).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID to token mapping for first training example (input)\n",
            "101 ----> [CLS]\n",
            "1996 ----> the\n",
            "3222 ----> commission\n",
            "1005 ----> '\n",
            "1055 ----> s\n",
            "6378 ----> proposal\n",
            "1010 ----> ,\n",
            "2174 ----> however\n",
            "1010 ----> ,\n",
            "2515 ----> does\n",
            "2025 ----> not\n",
            "2202 ----> take\n",
            "4070 ----> account\n",
            "1997 ----> of\n",
            "2035 ----> all\n",
            "1996 ----> the\n",
            "8866 ----> facts\n",
            "1010 ----> ,\n",
            "2107 ----> such\n",
            "2004 ----> as\n",
            "1996 ----> the\n",
            "3147 ----> cold\n",
            "4785 ----> climate\n",
            "2008 ----> that\n",
            "3653 ----> pre\n",
            "3567 ----> ##va\n",
            "12146 ----> ##ils\n",
            "1999 ----> in\n",
            "1996 ----> the\n",
            "2642 ----> northern\n",
            "4655 ----> regions\n",
            "1012 ----> .\n",
            "102 ----> [SEP]\n",
            "\n",
            "ID to token mapping for first training example (target)\n",
            "101 ----> [CLS]\n",
            "2474 ----> la\n",
            "14848 ----> proposition\n",
            "2139 ----> de\n",
            "2474 ----> la\n",
            "3222 ----> commission\n",
            "11265 ----> ne\n",
            "3653 ----> pre\n",
            "4859 ----> ##nd\n",
            "8292 ----> ce\n",
            "11837 ----> ##pen\n",
            "28210 ----> ##dant\n",
            "14674 ----> pas\n",
            "4372 ----> en\n",
            "9584 ----> consideration\n",
            "2000 ----> to\n",
            "2271 ----> ##us\n",
            "4649 ----> les\n",
            "2755 ----> fact\n",
            "26744 ----> ##eurs\n",
            "26785 ----> nec\n",
            "7971 ----> ##ess\n",
            "14737 ----> ##aire\n",
            "2015 ----> ##s\n",
            "1012 ----> .\n",
            "15333 ----> je\n",
            "25636 ----> pens\n",
            "2063 ----> ##e\n",
            "11968 ----> par\n",
            "4654 ----> ex\n",
            "6633 ----> ##em\n",
            "10814 ----> ##ple\n",
            "8740 ----> au\n",
            "18856 ----> cl\n",
            "9581 ----> ##ima\n",
            "2102 ----> ##t\n",
            "10424 ----> fr\n",
            "9314 ----> ##oid\n",
            "4078 ----> des\n",
            "4655 ----> regions\n",
            "17419 ----> sept\n",
            "4765 ----> ##ent\n",
            "14772 ----> ##rion\n",
            "23266 ----> ##ales\n",
            "1012 ----> .\n",
            "102 ----> [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-QqZ3lFo7rwW"
      },
      "source": [
        "## Model preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6f12798d-2aab-408f-b8af-807e666364c7",
        "id": "3FxkpKzR7rwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model = CombinedBertTransformerModel(\n",
        "    input_shape=train_dataset.as_numpy_iterator().next()[0].shape,\n",
        "    vocab_size=vocab_size,\n",
        "    num_layers=2,\n",
        "    units=32,\n",
        "    d_model=32,\n",
        "    num_heads=2,\n",
        "    dropout=0.2,\n",
        "    padding_label=0\n",
        ")\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-58b75539885e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpadding_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/uw-fsdl-bert-transfer-translation/CombinedBertTransformerModel.py\u001b[0m in \u001b[0;36mCombinedBertTransformerModel\u001b[0;34m(input_shape, vocab_size, num_layers, units, d_model, num_heads, dropout, padding_label)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0md_enc_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     )(inputs=[tokenized_output_sentence, bert_outputs, look_ahead_mask])\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/uw-fsdl-bert-transfer-translation/decoder.py\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(vocab_size, num_layers, units, d_model, d_enc_outputs, num_heads, dropout, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/uw-fsdl-bert-transfer-translation/positional_encoding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_model, position)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_POSITION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xJcFhGNI7-JW"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_6BZGmR7HlO",
        "colab": {}
      },
      "source": [
        "# Train and evaluate the model using tf.keras.Model.fit()\n",
        "# TODO This doesn't work yet.\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=2,\n",
        "    steps_per_epoch=115,\n",
        "    validation_data=validation_dataset,\n",
        "    validation_steps=7\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}